{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB4Vo2_anaBy"
      },
      "source": [
        "# Introduction to Pytorch\n",
        "TP Time : 3 hours\n",
        "\n",
        "In this TP we will introduce the Pytorch framework. We will see how to define a neural network, how to train it and how to use it to make predictions.\n",
        "We will implement two types of neural networks: a simple multi-layer perceptron and a convolutional neural network.\n",
        "Training will be done on the MNIST dataset, which is a dataset of handwritten digits. The goal is to be able to recognize the digit from an image of size 28x28 pixels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5_BvbiynaB4"
      },
      "source": [
        "## I - Multi-Layer Perceptron (MLP)\n",
        "\n",
        "MLP is a class of feedforward artificial neural network (ANN). A MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function.\n",
        "\n",
        "Lets start by importing the libraries we will need for this lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yHcIfL_naB5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Edn02n1SnaB7"
      },
      "source": [
        "### 1 - Dataset and DataLoader\n",
        "\n",
        "Now that we have the tools, let us define a function which allows us to load the MNIST data. For doing so, we need a dataset (`torch.utils.data.Dataset`) and a loader (`torch.utils.data.Dataloader`), allowing us to loop over the dataset. For MNIST PyTorch already contains a dataset definition, which you can find [here](https://pytorch.org/docs/stable/torchvision/datasets.html#mnist). For what concerns the dataloader, default ones can be found [here](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader). We must create train, validation and test set and a loader for each of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FP-fXOOTnaB8"
      },
      "outputs": [],
      "source": [
        "def get_data(batch_size, test_batch_size=256):\n",
        "  # This function is needed to convert the PIL images to Tensors\n",
        "  transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "\n",
        "  # Load data\n",
        "  mnist_dataset = torchvision.datasets.MNIST(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        transform=transform,\n",
        "        download=True\n",
        "    )\n",
        "  mnist_test = torchvision.datasets.MNIST(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        transform=transform,\n",
        "        download=True\n",
        "    )\n",
        "\n",
        "\n",
        "  # Create train and validation splits\n",
        "  dataset_size = len(mnist_dataset)\n",
        "  # We will use 80% of the data for training and 20% for validation\n",
        "  train_size = int(0.8 * dataset_size)\n",
        "  valid_size = dataset_size - train_size\n",
        "\n",
        "  mnist_train, mnist_valid = torch.utils.data.random_split(mnist_dataset, [train_size, valid_size])\n",
        "\n",
        "  # Initialize dataloaders\n",
        "  train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
        "  val_loader = DataLoader(mnist_valid, batch_size=batch_size, shuffle=False)\n",
        "  test_loader = DataLoader(mnist_test, batch_size=test_batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "  return train_loader, val_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N_5xAI1naB8"
      },
      "source": [
        "### 2 - Network definition\n",
        "\n",
        "Now that we have the data, what we need is a network. For now let us instantiate an MLP :\n",
        "1. 2 fully-connected layers (input-to-hidden and hidden-to-output).  The fully-connected layers are defined as `torch.nn.Linear`.  \n",
        "2. Between the layers we must put a non-linear activation. For now let us use a sigmoid (`torch.nn.Sigmoid`).\n",
        "3. For other layers and activation functions please have a look at the [doc](https://pytorch.org/docs/stable/nn.html).\n",
        "4. Do not forget that a network must extend a `torch.nn.Module`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VTg3arAnaB9"
      },
      "outputs": [],
      "source": [
        "# Our network\n",
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    super(MLP, self).__init__()\n",
        "    # TODO: Implement the constructor with the given parameters\n",
        "    self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Flatten the input\n",
        "    x = x.view(x.shape[0],-1)\n",
        "\n",
        "    # TODO: Implement the forward pass\n",
        "    x = self.fc1(x)\n",
        "    x = self.sigmoid(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiuXqgrHnaB9"
      },
      "source": [
        "### 3 - Loss / coast function\n",
        "\n",
        "To train the network, we obviously need a loss function. The task is classification with multiple classes, thus a proper loss could be a cross-entropy with softmax. We can again use `torch.nn` which contains several losses, among which `torch.nn.CrossEntropyLoss`.\n",
        "\n",
        "Notice that this loss already contains the softmax activation, thus we do not need to apply the softmax to the output of our network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ewpUBPSnaB-"
      },
      "outputs": [],
      "source": [
        "def get_cost_function():\n",
        "  cost_function = nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuHaIDpnnaB-"
      },
      "source": [
        "### 4 - Optimizer\n",
        "\n",
        "Now we must devise a way to update the parameters of our network. This can be easily held out by having a look at [`torch.optim`](https://pytorch.org/docs/stable/optim.html) which contains a large variety of optimizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcR4Px0ynaB-"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(model, lr, **kwargs):\n",
        "  optimizer = optim.SGD(model.parameters(), lr=lr, **kwargs)\n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJwGDaIdnaB_"
      },
      "source": [
        "### 5 - Train and test functions\n",
        "\n",
        "We are ready to merge everything by creating a training and test functions. Both of them must:\n",
        "\n",
        "1. Loop over the data (exploiting the dataloader, which is just an iterator)\n",
        "2. Forward the data through the network\n",
        "3. Comparing the output with the target labels for computing either the loss (train), the accuracy (test) or both.\n",
        "\n",
        "Additionally, during training we must:\n",
        "\n",
        "1. Compute the gradient with the backward pass (`loss.backward()`)\n",
        "2. Using the optimizer to update the weights (`optimizer.step()`)\n",
        "3. Cleaning the gradient of the weights in order to not accumulating it (`optimizer.zero_grad()`)\n",
        "\n",
        "With these steps in mind, we are ready to define everything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s71uMZknaB_"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "\n",
        "def train(model, data_loader, optimizer, cost_function, device='cuda'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  # Set the model in train mode\n",
        "  model.train()\n",
        "\n",
        "  progress_bar = tqdm.tqdm(data_loader, desc='Training', leave=False)\n",
        "\n",
        "  # Loop over the dataset using tqdm for progress bar\n",
        "  # Find a way to plot the loss on the tqdm bar\n",
        "  for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
        "\n",
        "      # Load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      # Zero the gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      # Apply the loss\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # Backward pass\n",
        "      loss.backward()\n",
        "\n",
        "      # Update parameters\n",
        "      optimizer.step()\n",
        "\n",
        "      # Update progress bar\n",
        "      samples += inputs.size(0)\n",
        "      cumulative_loss += loss.item()\n",
        "      _, predicted = outputs.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "      # Update progress bar description with loss\n",
        "      progress_bar.set_description(f'Training (Loss: {cumulative_loss / samples:.4f}, Acc: {cumulative_accuracy / samples * 100:.2f}%)')\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
        "\n",
        "\n",
        "def test(model, data_loader, cost_function, device='cuda'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  #Set the model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad(): # torch.no_grad() disables the autograd machinery, thus not saving the intermediate activations\n",
        "    # Loop over the dataset using tqdm for progress bar\n",
        "    progress_bar = tqdm.tqdm(data_loader, desc='Testing', leave=False)\n",
        "    # Find a way to plot the loss on the tqdm bar\n",
        "    for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
        "\n",
        "      # Load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      # Calculate the loss\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # Update progress bar\n",
        "      samples += inputs.size(0)\n",
        "      cumulative_loss += loss.item()\n",
        "      _, predicted = outputs.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "      # Update progress bar description with loss\n",
        "      progress_bar.set_description(f'Testing (Loss: {cumulative_loss / samples:.4f}, Acc: {cumulative_accuracy / samples * 100:.2f}%)')\n",
        "\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsi5kUU2naCA"
      },
      "source": [
        "### 6 - Trainer\n",
        "\n",
        "Finally, we need a main trainer function which initializes everything + the needed hyperparameters and loops over multiple epochs (printing the results)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0tUO0WRnaCA"
      },
      "outputs": [],
      "source": [
        "def trainer(batch_size=128, input_dim=28*28, hidden_dim=100, output_dim=10, device='cuda:0', learning_rate=0.01, epochs=10):\n",
        "  # TODO: Complete this initializations for dataset, model, optimizer and cost function\n",
        "  train_loader, val_loader, test_loader = get_data(batch_size=batch_size)\n",
        "  model = MLP(input_dim, hidden_dim, output_dim).to(device)\n",
        "  optimizer = get_optimizer(model, lr=learning_rate)\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  for e in range(epochs):\n",
        "    train_loss, train_accuracy = train(model, train_loader, optimizer, cost_function)\n",
        "    val_loss, val_accuracy = test(model, val_loader, cost_function)\n",
        "\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test(model, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(model, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(model, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "537VwFdqnaCB"
      },
      "source": [
        "## II - Convolutional Neural Networks\n",
        "\n",
        "In this section  we will learn how to train a CNN from scratch for classifying MNIST digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s34o6Kd-naCB"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq8wZhDjnaCB"
      },
      "source": [
        "### 1 - Define LeNet\n",
        "\n",
        "Here we are going to define our first CNN which is **LeNet** in this case. To construct a LeNet we will be using some convolutional layers followed by some fully-connected layers. The convolutional layers can be simply defined using `torch.nn.Conv2d` module of `torch.nn` package. Details can be found [here](https://pytorch.org/docs/stable/nn.html#conv2d). Moreover, we will use pooling operation to reduce the size of convolutional feature maps. For this case we are going to use `torch.nn.functional.max_pool2d`. Details about maxpooling can be found [here](https://pytorch.org/docs/stable/nn.html#max-pool2d)\n",
        "\n",
        "Differently from our previous Lab, we will use a Rectified Linear Units (ReLU) as activation function with the help of `torch.nn.functional.relu`, replacing `torch.nn.Sigmoid`. Details about ReLU can be found [here](https://pytorch.org/docs/stable/nn.html#id26)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1SdfGfNnaCB"
      },
      "outputs": [],
      "source": [
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "\n",
        "        # Define the convolutional layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)  # First Convolutional Layer\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)  # Second Convolutional Layer\n",
        "\n",
        "        # Define the fully connected layers\n",
        "        self.fc1 = nn.Linear(in_features=16 * 4 * 4, out_features=120)  # Fully Connected Layer 1\n",
        "        self.fc2 = nn.Linear(in_features=120, out_features=84)  # Fully Connected Layer 2\n",
        "        self.fc3 = nn.Linear(in_features=84, out_features=10)  # Output Layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolutional Layer 1 with ReLU activation and max pooling\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "\n",
        "        # Convolutional Layer 2 with ReLU activation and max pooling\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "\n",
        "        # Flatten the feature maps into a long vector\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Fully Connected Layers with ReLU activations\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        # Output Layer\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSnyOEExnaCC"
      },
      "source": [
        "### 2 - Define cost function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q_lmqM0naCC"
      },
      "outputs": [],
      "source": [
        "def get_cost_function():\n",
        "  cost_function = nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axM6TCianaCC"
      },
      "source": [
        "### 3 - Define the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1K2YJfH4naCC"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(model, lr, wd, momentum):\n",
        "  optimizer = optim.SGD(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=wd,\n",
        "        momentum=momentum\n",
        "    )\n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcmIKtMknaCC"
      },
      "source": [
        "### 4 - Train and test functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8hxbqVNnaCC"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "\n",
        "def train(model, data_loader, optimizer, cost_function, device='cuda'):\n",
        "    samples = 0.\n",
        "    cumulative_loss = 0.\n",
        "    cumulative_accuracy = 0.\n",
        "\n",
        "    # Set the model in train mode\n",
        "    model.train()\n",
        "\n",
        "    # Create a tqdm progress bar\n",
        "    progress_bar = tqdm.tqdm(data_loader, desc='Training', leave=False)\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
        "        # Load data into GPU\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = cost_function(outputs, targets)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update progress bar\n",
        "        samples += inputs.size(0)\n",
        "        cumulative_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "        # Update progress bar description with loss\n",
        "        progress_bar.set_description(f'Training (Loss: {cumulative_loss / samples:.4f}, Acc: {cumulative_accuracy / samples * 100:.2f}%)')\n",
        "\n",
        "    return cumulative_loss / samples, cumulative_accuracy / samples * 100\n",
        "\n",
        "\n",
        "def test(model, data_loader, cost_function, device='cuda'):\n",
        "    samples = 0.\n",
        "    cumulative_loss = 0.\n",
        "    cumulative_accuracy = 0.\n",
        "\n",
        "    # Set the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Create a tqdm progress bar\n",
        "        progress_bar = tqdm.tqdm(data_loader, desc='Testing', leave=False)\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
        "            # Load data into GPU\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calculate the loss\n",
        "            loss = cost_function(outputs, targets)\n",
        "\n",
        "            # Update progress bar\n",
        "            samples += inputs.size(0)\n",
        "            cumulative_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "            # Update progress bar description with loss\n",
        "            progress_bar.set_description(f'Testing (Loss: {cumulative_loss / samples:.4f}, Acc: {cumulative_accuracy / samples * 100:.2f}%)')\n",
        "\n",
        "    return cumulative_loss / samples, cumulative_accuracy / samples * 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nGfQ6y_naCC"
      },
      "source": [
        "### 5 - Dataset and Dataloader\n",
        "\n",
        "We will learn a new thing in this function as how to Normalize the inputs given to the network.\n",
        "\n",
        "***Why Normalization is needed***?\n",
        "\n",
        "To have nice and stable training of the network it is recommended to normalize the network inputs between \\[-1, 1\\].\n",
        "\n",
        "***How it can be done***?\n",
        "\n",
        "This can be simply done using `torchvision.transforms.Normalize()` transform. Details can be found [here](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Normalize)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ukhv0h2knaCC"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torch.utils.data\n",
        "\n",
        "def get_data(batch_size, test_batch_size=256):\n",
        "    # Prepare data transformations and then combine them sequentially\n",
        "    transform = [\n",
        "        T.ToTensor(),  # Converts Numpy to PyTorch Tensor\n",
        "        T.Normalize(mean=[0.5], std=[0.5])  # Normalizes the Tensors between [-1, 1]\n",
        "    ]\n",
        "    transform = T.Compose(transform)  # Composes the above transformations into one.\n",
        "\n",
        "    # Load data\n",
        "    mnist_dataset = torchvision.datasets.MNIST('./data', train=True, transform=transform, download=True)\n",
        "    mnist_test = torchvision.datasets.MNIST('./data', train=False, transform=transform, download=True)\n",
        "\n",
        "    # Create train and validation splits, we will take 80% of the training data for training and 20% for validation\n",
        "    dataset_size = len(mnist_dataset)\n",
        "    train_size = int(0.8 * dataset_size)\n",
        "    valid_size = dataset_size - train_size\n",
        "\n",
        "    mnist_train, mnist_valid = torch.utils.data.random_split(mnist_dataset, [train_size, valid_size])\n",
        "\n",
        "    # Initialize data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(mnist_valid, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=test_batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfRdSI8onaCD"
      },
      "source": [
        "### 6 - Trainer\n",
        "\n",
        "Finally, we need a main trainer function which initializes everything + the needed hyperparameters and loops over multiple epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBrGnEYLnaCD"
      },
      "outputs": [],
      "source": [
        "def main(batch_size=128,\n",
        "         device='cuda:0',\n",
        "         learning_rate=0.01,\n",
        "         weight_decay=0.000001,\n",
        "         momentum=0.9,\n",
        "         epochs=50):\n",
        "\n",
        "    # Load the dataset\n",
        "    train_loader, val_loader, test_loader = get_data(batch_size=batch_size)\n",
        "\n",
        "    # Define LeNet-5\n",
        "    model = LeNet().to(device)\n",
        "\n",
        "    # Define the optimizer\n",
        "    optimizer = get_optimizer(model, lr=learning_rate, wd=weight_decay, momentum=momentum)\n",
        "\n",
        "    # Define the cost function\n",
        "    cost_function = get_cost_function()\n",
        "\n",
        "    for e in range(epochs):\n",
        "        train_loss, train_accuracy = train(model, train_loader, optimizer, cost_function, device=device)\n",
        "        val_loss, val_accuracy = test(model, val_loader, cost_function, device=device)\n",
        "\n",
        "    print('After training:')\n",
        "    train_loss, train_accuracy = test(model, train_loader, cost_function, device=device)\n",
        "    val_loss, val_accuracy = test(model, val_loader, cost_function, device=device)\n",
        "    test_loss, test_accuracy = test(model, test_loader, cost_function, device=device)\n",
        "\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}